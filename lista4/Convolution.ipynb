{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL5Tb4rCFE2Q"
      },
      "source": [
        "Since the dataset remains to be the same, I will skip the minimal EDA, required in the previous coursework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPSYAYtrFPny"
      },
      "source": [
        "## Preprocess the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26jtAKSFFUim"
      },
      "source": [
        "### Download the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXkL-0qvZKxC",
        "outputId": "0d510ae3-a874-4d84-8821-3eccbe4c60ad"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        " transforms.ToTensor(),\n",
        " transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "trainset = datasets.FashionMNIST(\n",
        " root=\"./data\",\n",
        " train=True,\n",
        " download=True,\n",
        " transform=transform\n",
        ")\n",
        "testset = datasets.FashionMNIST(\n",
        " root=\"./data\",\n",
        " train=False,\n",
        " download=True,\n",
        " transform=transform\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37vXjT5-FX9r"
      },
      "source": [
        "### Create Validation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZeWbnFy6dK7z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.utils.data as torch_data\n",
        "\n",
        "val, train = torch_data.random_split(trainset, [int(len(trainset)*0.15), int(len(trainset)*0.85)], generator=torch.Generator().manual_seed(42))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD1LHb-iFe5b"
      },
      "source": [
        "### Create data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "S4vOHNm8dREk"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_loader = torch_data.DataLoader(train, BATCH_SIZE, shuffle=True)\n",
        "val_loader = torch_data.DataLoader(val, BATCH_SIZE)\n",
        "test_loader = torch_data.DataLoader(testset, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfy35jWsFkMn"
      },
      "source": [
        "### Create the Convolution Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WgOvHh3YdSvF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "\n",
        "class ParameterizedConvTwoD(nn.Module):\n",
        "\n",
        "  def __init__(self, img_height: int, img_width: int, num_classes: int, hidden_layers: list[int], use_batch_norm: bool, dropout_rate: float = 0.0,  in_channels: int = 1, pool: type[nn.Module] = None, use_gap: bool = True, stride: int = 2, padding: int = 1) -> None:\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.use_gap = use_gap\n",
        "    self.layers = nn.ModuleList()\n",
        "    self.current_in = in_channels\n",
        "    h, w = img_height, img_width\n",
        "\n",
        "\n",
        "    for h_layer in hidden_layers:\n",
        "\n",
        "      # init hidden layer\n",
        "\n",
        "      self.layers.append(nn.Conv2d(self.current_in, h_layer, kernel_size=3, padding=padding))\n",
        "\n",
        "      if (use_batch_norm):\n",
        "\n",
        "        self.layers.append(nn.BatchNorm2d(h_layer))\n",
        "\n",
        "      self.layers.append(nn.ReLU())\n",
        "\n",
        "      if pool:\n",
        "        self.layers.append(pool(kernel_size=2, stride=stride))\n",
        "        h //= 2\n",
        "        w //= 2\n",
        "\n",
        "      # if dropout\n",
        "\n",
        "      if dropout_rate > 0:\n",
        "        self.layers.append(nn.Dropout2d(dropout_rate))\n",
        "      self.current_in = h_layer\n",
        "\n",
        "    if self.use_gap:\n",
        "      self.gap_layer = nn.AdaptiveAvgPool2d((1, 1))\n",
        "      in_features = self.current_in\n",
        "    else:\n",
        "      self.gap_layer = None\n",
        "      in_features = self.current_in * h * w\n",
        "\n",
        "    self.classifier = nn.Linear(in_features, num_classes)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "\n",
        "      x = layer(x)\n",
        "\n",
        "    if self.use_gap:\n",
        "      x = self.gap_layer(x)\n",
        "      \n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.classifier(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qBcPDg-FpZ1"
      },
      "source": [
        "### Set up a seed for output reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MMYswualErrw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def set_seed(seed=42):\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "\n",
        "set_seed()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPYWlq8RFr78"
      },
      "source": [
        "### Set up Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CFrB29y4FsgF"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKxr5R7eFyY9"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gAqB-QoF4yd"
      },
      "source": [
        "### Training Params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ubjiUSRNF63p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Set training params up\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "patience = 8 # early stopping param - how many epochs to wait for an improvement\n",
        "\n",
        "# Calculate input size\n",
        "image_shape = trainset[0][0].shape\n",
        "input_size = image_shape[1] * image_shape[2]\n",
        "\n",
        "# Set up model config\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "best_model_path = 'models/best_model.pth'\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# Set up metric tracking\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "train_f1_history = []\n",
        "val_f1_history = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YkDiCbB7GeS-"
      },
      "outputs": [],
      "source": [
        "def training_loop(epochs, model, optimizer, criterion, patience, writer, early_stopping, lr_scheduler=None):\n",
        "  if early_stopping:\n",
        "    patience_counter = 0\n",
        "    best_val_loss = float('inf')\n",
        "    # Training loop\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    all_train_predictions = []\n",
        "    all_train_labels = []\n",
        "\n",
        "    for train_images, train_labels in train_loader:\n",
        "      # Move to cuda's vram, if cuda avaliable (torch handles that)\n",
        "      train_images, train_labels = train_images.to(device), train_labels.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      train_outputs = model(train_images)\n",
        "      _, predicted = torch.max(train_outputs, dim=1)\n",
        "      correct_predictions += (predicted == train_labels).sum().item()\n",
        "      total_predictions += train_labels.size(0)\n",
        "\n",
        "      train_loss = criterion(train_outputs, train_labels)\n",
        "      running_loss += train_loss.item()\n",
        "\n",
        "      all_train_predictions.extend(predicted.cpu().numpy())\n",
        "      all_train_labels.extend(train_labels.cpu().numpy())\n",
        "\n",
        "      train_loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "\n",
        "    # calculate avg loss\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    train_loss_history.append(avg_train_loss)\n",
        "    train_f1 = f1_score(y_pred=all_train_predictions, y_true=all_train_labels, average='macro')\n",
        "    train_f1_history.append(train_f1)\n",
        "\n",
        "    all_val_predictions = []\n",
        "    all_val_labels = []\n",
        "\n",
        "    # eval\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      val_running_loss = 0.0\n",
        "      correct_predictions = 0\n",
        "      total_predictions = 0\n",
        "\n",
        "      for val_images, val_labels in val_loader:\n",
        "        val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
        "        val_outputs = model(val_images)\n",
        "        _, predicted = torch.max(val_outputs, dim=1)\n",
        "        correct_predictions += (predicted == val_labels).sum().item()\n",
        "        total_predictions += val_labels.size(0)\n",
        "\n",
        "        val_loss = criterion(val_outputs, val_labels)\n",
        "        val_running_loss += val_loss.item()\n",
        "\n",
        "        all_val_predictions.extend(predicted.cpu().numpy())\n",
        "        all_val_labels.extend(val_labels.cpu().numpy())\n",
        "\n",
        "      avg_val_loss = val_running_loss / len(val_loader)\n",
        "      val_loss_history.append(avg_val_loss)\n",
        "      val_f1 = f1_score(y_pred=all_val_predictions, y_true=all_val_labels, average='macro')\n",
        "      val_f1_history.append(val_f1)\n",
        "\n",
        "    # Step the scheduler if provided\n",
        "    if lr_scheduler is not None:\n",
        "        if isinstance(lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "            lr_scheduler.step(avg_val_loss)  # Needs metric\n",
        "        else:\n",
        "            lr_scheduler.step()  # StepLR, etc. don't need metric\n",
        "\n",
        "        # Log current learning rate to TensorBoard\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        writer.add_scalar('Learning_Rate', current_lr, epoch)\n",
        "\n",
        "    # log the stats to tensorboard\n",
        "    writer.add_scalar('Loss/train', avg_train_loss, epoch)\n",
        "    writer.add_scalar('Loss/val', avg_val_loss, epoch)\n",
        "    writer.add_scalar(\"F1/val\", val_f1, epoch)\n",
        "    writer.add_scalar(\"F1/train\", train_f1, epoch)\n",
        "\n",
        "    # early stopping logic\n",
        "    if (early_stopping):\n",
        "      if (avg_val_loss < best_val_loss):\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        patience_counter = 0\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Model saved! Val loss improved to {best_val_loss:.4f}\")\n",
        "      else:\n",
        "        patience_counter += 1\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - No improvement. Patience: {patience_counter}/{patience}\")\n",
        "\n",
        "      if (patience_counter >= patience):\n",
        "        print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "        break\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Train F1: {train_f1:.4f}, Val Loss: {avg_val_loss:.4f}, Val F1: {val_f1:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQFU8-lzIK3m"
      },
      "source": [
        "### Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "bjDFpcVvINiu",
        "outputId": "0c410fac-3549-40e2-ece2-ece3f6e23903"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 - Model saved! Val loss improved to 1.1256\n",
            "Epoch 1/20, Train Loss: 1.4635, Train F1: 0.4706, Val Loss: 1.1256, Val F1: 0.5775\n",
            "Epoch 2/20 - Model saved! Val loss improved to 1.0156\n",
            "Epoch 2/20, Train Loss: 1.0647, Train F1: 0.6119, Val Loss: 1.0156, Val F1: 0.6083\n",
            "Epoch 3/20 - Model saved! Val loss improved to 0.9576\n",
            "Epoch 3/20, Train Loss: 0.9868, Train F1: 0.6415, Val Loss: 0.9576, Val F1: 0.6397\n",
            "Epoch 4/20 - Model saved! Val loss improved to 0.8912\n",
            "Epoch 4/20, Train Loss: 0.9332, Train F1: 0.6630, Val Loss: 0.8912, Val F1: 0.6785\n",
            "Epoch 5/20 - Model saved! Val loss improved to 0.8575\n",
            "Epoch 5/20, Train Loss: 0.8834, Train F1: 0.6859, Val Loss: 0.8575, Val F1: 0.6855\n",
            "Epoch 6/20 - Model saved! Val loss improved to 0.7941\n",
            "Epoch 6/20, Train Loss: 0.8291, Train F1: 0.7080, Val Loss: 0.7941, Val F1: 0.7136\n",
            "Epoch 7/20 - Model saved! Val loss improved to 0.7383\n",
            "Epoch 7/20, Train Loss: 0.7810, Train F1: 0.7297, Val Loss: 0.7383, Val F1: 0.7451\n",
            "Epoch 8/20 - Model saved! Val loss improved to 0.7063\n",
            "Epoch 8/20, Train Loss: 0.7397, Train F1: 0.7442, Val Loss: 0.7063, Val F1: 0.7572\n",
            "Epoch 9/20 - Model saved! Val loss improved to 0.6742\n",
            "Epoch 9/20, Train Loss: 0.7053, Train F1: 0.7569, Val Loss: 0.6742, Val F1: 0.7639\n",
            "Epoch 10/20 - Model saved! Val loss improved to 0.6654\n",
            "Epoch 10/20, Train Loss: 0.6823, Train F1: 0.7644, Val Loss: 0.6654, Val F1: 0.7680\n",
            "Epoch 11/20 - Model saved! Val loss improved to 0.6434\n",
            "Epoch 11/20, Train Loss: 0.6591, Train F1: 0.7717, Val Loss: 0.6434, Val F1: 0.7762\n",
            "Epoch 12/20 - Model saved! Val loss improved to 0.6216\n",
            "Epoch 12/20, Train Loss: 0.6455, Train F1: 0.7767, Val Loss: 0.6216, Val F1: 0.7847\n",
            "Epoch 13/20 - Model saved! Val loss improved to 0.6015\n",
            "Epoch 13/20, Train Loss: 0.6327, Train F1: 0.7824, Val Loss: 0.6015, Val F1: 0.7907\n",
            "Epoch 14/20 - No improvement. Patience: 1/8\n",
            "Epoch 14/20, Train Loss: 0.6191, Train F1: 0.7864, Val Loss: 0.6088, Val F1: 0.7869\n",
            "Epoch 15/20 - Model saved! Val loss improved to 0.5889\n",
            "Epoch 15/20, Train Loss: 0.6131, Train F1: 0.7868, Val Loss: 0.5889, Val F1: 0.7926\n",
            "Epoch 16/20 - Model saved! Val loss improved to 0.5824\n",
            "Epoch 16/20, Train Loss: 0.6021, Train F1: 0.7912, Val Loss: 0.5824, Val F1: 0.7909\n",
            "Epoch 17/20 - Model saved! Val loss improved to 0.5806\n",
            "Epoch 17/20, Train Loss: 0.5963, Train F1: 0.7919, Val Loss: 0.5806, Val F1: 0.7975\n",
            "Epoch 18/20 - Model saved! Val loss improved to 0.5790\n",
            "Epoch 18/20, Train Loss: 0.5889, Train F1: 0.7957, Val Loss: 0.5790, Val F1: 0.7949\n",
            "Epoch 19/20 - No improvement. Patience: 1/8\n",
            "Epoch 19/20, Train Loss: 0.5827, Train F1: 0.7994, Val Loss: 0.5844, Val F1: 0.7791\n",
            "Epoch 20/20 - Model saved! Val loss improved to 0.5715\n",
            "Epoch 20/20, Train Loss: 0.5747, Train F1: 0.8001, Val Loss: 0.5715, Val F1: 0.7965\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 28, 28]             640\n",
            "              ReLU-2           [-1, 64, 28, 28]               0\n",
            "            Conv2d-3           [-1, 32, 28, 28]          18,464\n",
            "              ReLU-4           [-1, 32, 28, 28]               0\n",
            " AdaptiveAvgPool2d-5             [-1, 32, 1, 1]               0\n",
            "            Linear-6                   [-1, 10]             330\n",
            "================================================================\n",
            "Total params: 19,434\n",
            "Trainable params: 19,434\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.15\n",
            "Params size (MB): 0.07\n",
            "Estimated Total Size (MB): 1.23\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "# Set model up\n",
        "LR = 0.001\n",
        "EPOCHS = 20\n",
        "hidden_layers = [64,32]\n",
        "use_batch_norm = False\n",
        "early_stopping = True\n",
        "\n",
        "model = ParameterizedConvTwoD(\n",
        "    img_height=28,\n",
        "    img_width=28,\n",
        "    num_classes=10,\n",
        "    hidden_layers=hidden_layers,\n",
        "    use_batch_norm=use_batch_norm,\n",
        ")\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "model.to(device)\n",
        "writer = SummaryWriter(log_dir='runs/baseline')\n",
        "\n",
        "training_loop(epochs=EPOCHS, criterion=criterion, model=model, optimizer=optimizer, patience=patience, writer=writer, early_stopping=early_stopping)\n",
        "summary(model, input_size=(1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7Bx168ohQRT"
      },
      "source": [
        "### Max Pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2gS62RBvhSQA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 - Model saved! Val loss improved to 0.8840\n",
            "Epoch 1/20, Train Loss: 1.2317, Train F1: 0.5698, Val Loss: 0.8840, Val F1: 0.6781\n",
            "Epoch 2/20 - Model saved! Val loss improved to 0.7563\n",
            "Epoch 2/20, Train Loss: 0.8147, Train F1: 0.7088, Val Loss: 0.7563, Val F1: 0.7187\n",
            "Epoch 3/20 - Model saved! Val loss improved to 0.6920\n",
            "Epoch 3/20, Train Loss: 0.7155, Train F1: 0.7401, Val Loss: 0.6920, Val F1: 0.7373\n",
            "Epoch 4/20 - Model saved! Val loss improved to 0.6365\n",
            "Epoch 4/20, Train Loss: 0.6558, Train F1: 0.7634, Val Loss: 0.6365, Val F1: 0.7711\n",
            "Epoch 5/20 - Model saved! Val loss improved to 0.6144\n",
            "Epoch 5/20, Train Loss: 0.6198, Train F1: 0.7749, Val Loss: 0.6144, Val F1: 0.7638\n",
            "Epoch 6/20 - Model saved! Val loss improved to 0.5703\n",
            "Epoch 6/20, Train Loss: 0.5847, Train F1: 0.7886, Val Loss: 0.5703, Val F1: 0.7986\n",
            "Epoch 7/20 - No improvement. Patience: 1/8\n",
            "Epoch 7/20, Train Loss: 0.5635, Train F1: 0.7987, Val Loss: 0.5721, Val F1: 0.7994\n",
            "Epoch 8/20 - Model saved! Val loss improved to 0.5326\n",
            "Epoch 8/20, Train Loss: 0.5402, Train F1: 0.8058, Val Loss: 0.5326, Val F1: 0.8057\n",
            "Epoch 9/20 - Model saved! Val loss improved to 0.5286\n",
            "Epoch 9/20, Train Loss: 0.5229, Train F1: 0.8144, Val Loss: 0.5286, Val F1: 0.8069\n",
            "Epoch 10/20 - Model saved! Val loss improved to 0.4953\n",
            "Epoch 10/20, Train Loss: 0.5089, Train F1: 0.8201, Val Loss: 0.4953, Val F1: 0.8257\n",
            "Epoch 11/20 - No improvement. Patience: 1/8\n",
            "Epoch 11/20, Train Loss: 0.4967, Train F1: 0.8245, Val Loss: 0.5136, Val F1: 0.8170\n",
            "Epoch 12/20 - Model saved! Val loss improved to 0.4711\n",
            "Epoch 12/20, Train Loss: 0.4855, Train F1: 0.8278, Val Loss: 0.4711, Val F1: 0.8366\n",
            "Epoch 13/20 - No improvement. Patience: 1/8\n",
            "Epoch 13/20, Train Loss: 0.4739, Train F1: 0.8319, Val Loss: 0.4908, Val F1: 0.8313\n",
            "Epoch 14/20 - Model saved! Val loss improved to 0.4688\n",
            "Epoch 14/20, Train Loss: 0.4667, Train F1: 0.8345, Val Loss: 0.4688, Val F1: 0.8323\n",
            "Epoch 15/20 - Model saved! Val loss improved to 0.4488\n",
            "Epoch 15/20, Train Loss: 0.4571, Train F1: 0.8383, Val Loss: 0.4488, Val F1: 0.8421\n",
            "Epoch 16/20 - No improvement. Patience: 1/8\n",
            "Epoch 16/20, Train Loss: 0.4502, Train F1: 0.8390, Val Loss: 0.4943, Val F1: 0.8241\n",
            "Epoch 17/20 - Model saved! Val loss improved to 0.4444\n",
            "Epoch 17/20, Train Loss: 0.4417, Train F1: 0.8441, Val Loss: 0.4444, Val F1: 0.8447\n",
            "Epoch 18/20 - Model saved! Val loss improved to 0.4252\n",
            "Epoch 18/20, Train Loss: 0.4357, Train F1: 0.8458, Val Loss: 0.4252, Val F1: 0.8535\n",
            "Epoch 19/20 - No improvement. Patience: 1/8\n",
            "Epoch 19/20, Train Loss: 0.4285, Train F1: 0.8486, Val Loss: 0.4266, Val F1: 0.8485\n",
            "Epoch 20/20 - Model saved! Val loss improved to 0.4222\n",
            "Epoch 20/20, Train Loss: 0.4233, Train F1: 0.8484, Val Loss: 0.4222, Val F1: 0.8518\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 28, 28]             640\n",
            "              ReLU-2           [-1, 64, 28, 28]               0\n",
            "         MaxPool2d-3           [-1, 64, 14, 14]               0\n",
            "            Conv2d-4           [-1, 32, 14, 14]          18,464\n",
            "              ReLU-5           [-1, 32, 14, 14]               0\n",
            "         MaxPool2d-6             [-1, 32, 7, 7]               0\n",
            " AdaptiveAvgPool2d-7             [-1, 32, 1, 1]               0\n",
            "            Linear-8                   [-1, 10]             330\n",
            "================================================================\n",
            "Total params: 19,434\n",
            "Trainable params: 19,434\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.97\n",
            "Params size (MB): 0.07\n",
            "Estimated Total Size (MB): 1.05\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "pool = nn.MaxPool2d\n",
        "\n",
        "model = ParameterizedConvTwoD(\n",
        "    img_height=28,\n",
        "    img_width=28,\n",
        "    num_classes=10,\n",
        "    hidden_layers=hidden_layers,\n",
        "    use_batch_norm=use_batch_norm,\n",
        "    pool=pool\n",
        ")\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "writer = SummaryWriter(log_dir='runs/max_pooling')\n",
        "\n",
        "training_loop(epochs=EPOCHS, criterion=criterion, model=model, optimizer=optimizer, patience=patience, writer=writer, early_stopping=early_stopping)\n",
        "summary(model, input_size=(1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEcCYpQMhSe5"
      },
      "source": [
        "### Average Pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mSBJfNOiiSC9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 - Model saved! Val loss improved to 1.0602\n",
            "Epoch 1/20, Train Loss: 1.3857, Train F1: 0.5006, Val Loss: 1.0602, Val F1: 0.6168\n",
            "Epoch 2/20 - Model saved! Val loss improved to 0.8914\n",
            "Epoch 2/20, Train Loss: 0.9552, Train F1: 0.6544, Val Loss: 0.8914, Val F1: 0.6509\n",
            "Epoch 3/20 - Model saved! Val loss improved to 0.7989\n",
            "Epoch 3/20, Train Loss: 0.8348, Train F1: 0.6970, Val Loss: 0.7989, Val F1: 0.7016\n",
            "Epoch 4/20 - Model saved! Val loss improved to 0.7494\n",
            "Epoch 4/20, Train Loss: 0.7692, Train F1: 0.7241, Val Loss: 0.7494, Val F1: 0.7306\n",
            "Epoch 5/20 - Model saved! Val loss improved to 0.7001\n",
            "Epoch 5/20, Train Loss: 0.7264, Train F1: 0.7384, Val Loss: 0.7001, Val F1: 0.7504\n",
            "Epoch 6/20 - Model saved! Val loss improved to 0.6925\n",
            "Epoch 6/20, Train Loss: 0.6973, Train F1: 0.7495, Val Loss: 0.6925, Val F1: 0.7535\n",
            "Epoch 7/20 - Model saved! Val loss improved to 0.6562\n",
            "Epoch 7/20, Train Loss: 0.6697, Train F1: 0.7619, Val Loss: 0.6562, Val F1: 0.7635\n",
            "Epoch 8/20 - Model saved! Val loss improved to 0.6379\n",
            "Epoch 8/20, Train Loss: 0.6483, Train F1: 0.7671, Val Loss: 0.6379, Val F1: 0.7738\n",
            "Epoch 9/20 - Model saved! Val loss improved to 0.6168\n",
            "Epoch 9/20, Train Loss: 0.6319, Train F1: 0.7730, Val Loss: 0.6168, Val F1: 0.7708\n",
            "Epoch 10/20 - Model saved! Val loss improved to 0.5985\n",
            "Epoch 10/20, Train Loss: 0.6125, Train F1: 0.7806, Val Loss: 0.5985, Val F1: 0.7879\n",
            "Epoch 11/20 - Model saved! Val loss improved to 0.5899\n",
            "Epoch 11/20, Train Loss: 0.5992, Train F1: 0.7853, Val Loss: 0.5899, Val F1: 0.7934\n",
            "Epoch 12/20 - No improvement. Patience: 1/8\n",
            "Epoch 12/20, Train Loss: 0.5876, Train F1: 0.7910, Val Loss: 0.6032, Val F1: 0.7855\n",
            "Epoch 13/20 - Model saved! Val loss improved to 0.5664\n",
            "Epoch 13/20, Train Loss: 0.5744, Train F1: 0.7946, Val Loss: 0.5664, Val F1: 0.8032\n",
            "Epoch 14/20 - Model saved! Val loss improved to 0.5616\n",
            "Epoch 14/20, Train Loss: 0.5649, Train F1: 0.7977, Val Loss: 0.5616, Val F1: 0.8023\n",
            "Epoch 15/20 - Model saved! Val loss improved to 0.5395\n",
            "Epoch 15/20, Train Loss: 0.5554, Train F1: 0.8037, Val Loss: 0.5395, Val F1: 0.8112\n",
            "Epoch 16/20 - Model saved! Val loss improved to 0.5273\n",
            "Epoch 16/20, Train Loss: 0.5488, Train F1: 0.8035, Val Loss: 0.5273, Val F1: 0.8122\n",
            "Epoch 17/20 - No improvement. Patience: 1/8\n",
            "Epoch 17/20, Train Loss: 0.5397, Train F1: 0.8088, Val Loss: 0.5367, Val F1: 0.8100\n",
            "Epoch 18/20 - Model saved! Val loss improved to 0.5187\n",
            "Epoch 18/20, Train Loss: 0.5324, Train F1: 0.8087, Val Loss: 0.5187, Val F1: 0.8135\n",
            "Epoch 19/20 - No improvement. Patience: 1/8\n",
            "Epoch 19/20, Train Loss: 0.5268, Train F1: 0.8119, Val Loss: 0.5250, Val F1: 0.8165\n",
            "Epoch 20/20 - Model saved! Val loss improved to 0.5132\n",
            "Epoch 20/20, Train Loss: 0.5189, Train F1: 0.8162, Val Loss: 0.5132, Val F1: 0.8170\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 28, 28]             640\n",
            "              ReLU-2           [-1, 64, 28, 28]               0\n",
            "         AvgPool2d-3           [-1, 64, 14, 14]               0\n",
            "            Conv2d-4           [-1, 32, 14, 14]          18,464\n",
            "              ReLU-5           [-1, 32, 14, 14]               0\n",
            "         AvgPool2d-6             [-1, 32, 7, 7]               0\n",
            " AdaptiveAvgPool2d-7             [-1, 32, 1, 1]               0\n",
            "            Linear-8                   [-1, 10]             330\n",
            "================================================================\n",
            "Total params: 19,434\n",
            "Trainable params: 19,434\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.97\n",
            "Params size (MB): 0.07\n",
            "Estimated Total Size (MB): 1.05\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "pool = nn.AvgPool2d\n",
        "\n",
        "model = ParameterizedConvTwoD(\n",
        "    img_height=28,\n",
        "    img_width=28,\n",
        "    num_classes=10,\n",
        "    hidden_layers=hidden_layers,\n",
        "    use_batch_norm=use_batch_norm,\n",
        "    pool=pool\n",
        ")\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "writer = SummaryWriter(log_dir='runs/avg_pooling')\n",
        "\n",
        "training_loop(epochs=EPOCHS, criterion=criterion, model=model, optimizer=optimizer, patience=patience, writer=writer, early_stopping=early_stopping)\n",
        "summary(model, input_size=(1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Global Average Pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 - Model saved! Val loss improved to 1.0344\n",
            "Epoch 1/20, Train Loss: 1.3647, Train F1: 0.5051, Val Loss: 1.0344, Val F1: 0.6131\n",
            "Epoch 2/20 - Model saved! Val loss improved to 0.9080\n",
            "Epoch 2/20, Train Loss: 0.9531, Train F1: 0.6541, Val Loss: 0.9080, Val F1: 0.6738\n",
            "Epoch 3/20 - Model saved! Val loss improved to 0.8122\n",
            "Epoch 3/20, Train Loss: 0.8552, Train F1: 0.6919, Val Loss: 0.8122, Val F1: 0.6886\n",
            "Epoch 4/20 - Model saved! Val loss improved to 0.7592\n",
            "Epoch 4/20, Train Loss: 0.7965, Train F1: 0.7147, Val Loss: 0.7592, Val F1: 0.7229\n",
            "Epoch 5/20 - Model saved! Val loss improved to 0.7403\n",
            "Epoch 5/20, Train Loss: 0.7532, Train F1: 0.7314, Val Loss: 0.7403, Val F1: 0.7400\n",
            "Epoch 6/20 - Model saved! Val loss improved to 0.6839\n",
            "Epoch 6/20, Train Loss: 0.7148, Train F1: 0.7483, Val Loss: 0.6839, Val F1: 0.7547\n",
            "Epoch 7/20 - Model saved! Val loss improved to 0.6595\n",
            "Epoch 7/20, Train Loss: 0.6877, Train F1: 0.7577, Val Loss: 0.6595, Val F1: 0.7651\n",
            "Epoch 8/20 - Model saved! Val loss improved to 0.6527\n",
            "Epoch 8/20, Train Loss: 0.6642, Train F1: 0.7647, Val Loss: 0.6527, Val F1: 0.7625\n",
            "Epoch 9/20 - Model saved! Val loss improved to 0.6382\n",
            "Epoch 9/20, Train Loss: 0.6429, Train F1: 0.7737, Val Loss: 0.6382, Val F1: 0.7686\n",
            "Epoch 10/20 - Model saved! Val loss improved to 0.6154\n",
            "Epoch 10/20, Train Loss: 0.6267, Train F1: 0.7779, Val Loss: 0.6154, Val F1: 0.7787\n",
            "Epoch 11/20 - Model saved! Val loss improved to 0.6122\n",
            "Epoch 11/20, Train Loss: 0.6094, Train F1: 0.7846, Val Loss: 0.6122, Val F1: 0.7812\n",
            "Epoch 12/20 - Model saved! Val loss improved to 0.5980\n",
            "Epoch 12/20, Train Loss: 0.5954, Train F1: 0.7900, Val Loss: 0.5980, Val F1: 0.7804\n",
            "Epoch 13/20 - Model saved! Val loss improved to 0.5616\n",
            "Epoch 13/20, Train Loss: 0.5838, Train F1: 0.7939, Val Loss: 0.5616, Val F1: 0.7963\n",
            "Epoch 14/20 - No improvement. Patience: 1/8\n",
            "Epoch 14/20, Train Loss: 0.5734, Train F1: 0.7972, Val Loss: 0.5787, Val F1: 0.7899\n",
            "Epoch 15/20 - Model saved! Val loss improved to 0.5476\n",
            "Epoch 15/20, Train Loss: 0.5612, Train F1: 0.8036, Val Loss: 0.5476, Val F1: 0.8020\n",
            "Epoch 16/20 - Model saved! Val loss improved to 0.5435\n",
            "Epoch 16/20, Train Loss: 0.5533, Train F1: 0.8039, Val Loss: 0.5435, Val F1: 0.8037\n",
            "Epoch 17/20 - Model saved! Val loss improved to 0.5256\n",
            "Epoch 17/20, Train Loss: 0.5444, Train F1: 0.8079, Val Loss: 0.5256, Val F1: 0.8092\n",
            "Epoch 18/20 - No improvement. Patience: 1/8\n",
            "Epoch 18/20, Train Loss: 0.5374, Train F1: 0.8103, Val Loss: 0.5313, Val F1: 0.8078\n",
            "Epoch 19/20 - Model saved! Val loss improved to 0.5202\n",
            "Epoch 19/20, Train Loss: 0.5292, Train F1: 0.8122, Val Loss: 0.5202, Val F1: 0.8180\n",
            "Epoch 20/20 - Model saved! Val loss improved to 0.5075\n",
            "Epoch 20/20, Train Loss: 0.5214, Train F1: 0.8156, Val Loss: 0.5075, Val F1: 0.8205\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 28, 28]             640\n",
            "              ReLU-2           [-1, 64, 28, 28]               0\n",
            "         AvgPool2d-3           [-1, 64, 14, 14]               0\n",
            "            Conv2d-4           [-1, 32, 14, 14]          18,464\n",
            "              ReLU-5           [-1, 32, 14, 14]               0\n",
            "         AvgPool2d-6             [-1, 32, 7, 7]               0\n",
            " AdaptiveAvgPool2d-7             [-1, 32, 1, 1]               0\n",
            "            Linear-8                   [-1, 10]             330\n",
            "================================================================\n",
            "Total params: 19,434\n",
            "Trainable params: 19,434\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.97\n",
            "Params size (MB): 0.07\n",
            "Estimated Total Size (MB): 1.05\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "use_gap = True\n",
        "\n",
        "model = ParameterizedConvTwoD(\n",
        "    img_height=28,\n",
        "    img_width=28,\n",
        "    num_classes=10,\n",
        "    hidden_layers=hidden_layers,\n",
        "    use_batch_norm=use_batch_norm,\n",
        "    pool=pool,\n",
        "    use_gap=use_gap\n",
        ")\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "writer = SummaryWriter(log_dir='runs/gap')\n",
        "\n",
        "training_loop(epochs=EPOCHS, criterion=criterion, model=model, optimizer=optimizer, patience=patience, writer=writer, early_stopping=early_stopping)\n",
        "summary(model, input_size=(1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dropout & Batch Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 - Model saved! Val loss improved to 1.0839\n",
            "Epoch 1/20, Train Loss: 1.4736, Train F1: 0.4782, Val Loss: 1.0839, Val F1: 0.6518\n",
            "Epoch 2/20 - Model saved! Val loss improved to 0.9257\n",
            "Epoch 2/20, Train Loss: 1.1346, Train F1: 0.5860, Val Loss: 0.9257, Val F1: 0.7044\n",
            "Epoch 3/20 - Model saved! Val loss improved to 0.8308\n",
            "Epoch 3/20, Train Loss: 1.0283, Train F1: 0.6223, Val Loss: 0.8308, Val F1: 0.7150\n",
            "Epoch 4/20 - Model saved! Val loss improved to 0.7673\n",
            "Epoch 4/20, Train Loss: 0.9585, Train F1: 0.6486, Val Loss: 0.7673, Val F1: 0.7396\n",
            "Epoch 5/20 - Model saved! Val loss improved to 0.7223\n",
            "Epoch 5/20, Train Loss: 0.9060, Train F1: 0.6687, Val Loss: 0.7223, Val F1: 0.7488\n",
            "Epoch 6/20 - Model saved! Val loss improved to 0.6860\n",
            "Epoch 6/20, Train Loss: 0.8608, Train F1: 0.6857, Val Loss: 0.6860, Val F1: 0.7746\n",
            "Epoch 7/20 - Model saved! Val loss improved to 0.6687\n",
            "Epoch 7/20, Train Loss: 0.8273, Train F1: 0.7001, Val Loss: 0.6687, Val F1: 0.7799\n",
            "Epoch 8/20 - Model saved! Val loss improved to 0.6305\n",
            "Epoch 8/20, Train Loss: 0.8009, Train F1: 0.7091, Val Loss: 0.6305, Val F1: 0.7866\n",
            "Epoch 9/20 - Model saved! Val loss improved to 0.6062\n",
            "Epoch 9/20, Train Loss: 0.7742, Train F1: 0.7226, Val Loss: 0.6062, Val F1: 0.7962\n",
            "Epoch 10/20 - Model saved! Val loss improved to 0.5919\n",
            "Epoch 10/20, Train Loss: 0.7595, Train F1: 0.7271, Val Loss: 0.5919, Val F1: 0.7974\n",
            "Epoch 11/20 - Model saved! Val loss improved to 0.5736\n",
            "Epoch 11/20, Train Loss: 0.7344, Train F1: 0.7338, Val Loss: 0.5736, Val F1: 0.8060\n",
            "Epoch 12/20 - Model saved! Val loss improved to 0.5660\n",
            "Epoch 12/20, Train Loss: 0.7196, Train F1: 0.7422, Val Loss: 0.5660, Val F1: 0.8082\n",
            "Epoch 13/20 - Model saved! Val loss improved to 0.5584\n",
            "Epoch 13/20, Train Loss: 0.7094, Train F1: 0.7465, Val Loss: 0.5584, Val F1: 0.8159\n",
            "Epoch 14/20 - Model saved! Val loss improved to 0.5433\n",
            "Epoch 14/20, Train Loss: 0.6938, Train F1: 0.7531, Val Loss: 0.5433, Val F1: 0.8156\n",
            "Epoch 15/20 - Model saved! Val loss improved to 0.5391\n",
            "Epoch 15/20, Train Loss: 0.6853, Train F1: 0.7541, Val Loss: 0.5391, Val F1: 0.8156\n",
            "Epoch 16/20 - Model saved! Val loss improved to 0.5344\n",
            "Epoch 16/20, Train Loss: 0.6685, Train F1: 0.7612, Val Loss: 0.5344, Val F1: 0.8198\n",
            "Epoch 17/20 - Model saved! Val loss improved to 0.5319\n",
            "Epoch 17/20, Train Loss: 0.6614, Train F1: 0.7645, Val Loss: 0.5319, Val F1: 0.8268\n",
            "Epoch 18/20 - Model saved! Val loss improved to 0.5097\n",
            "Epoch 18/20, Train Loss: 0.6542, Train F1: 0.7673, Val Loss: 0.5097, Val F1: 0.8294\n",
            "Epoch 19/20 - Model saved! Val loss improved to 0.5097\n",
            "Epoch 19/20, Train Loss: 0.6453, Train F1: 0.7720, Val Loss: 0.5097, Val F1: 0.8326\n",
            "Epoch 20/20 - Model saved! Val loss improved to 0.4987\n",
            "Epoch 20/20, Train Loss: 0.6387, Train F1: 0.7725, Val Loss: 0.4987, Val F1: 0.8325\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 28, 28]             640\n",
            "       BatchNorm2d-2           [-1, 64, 28, 28]             128\n",
            "              ReLU-3           [-1, 64, 28, 28]               0\n",
            "         AvgPool2d-4           [-1, 64, 14, 14]               0\n",
            "         Dropout2d-5           [-1, 64, 14, 14]               0\n",
            "            Conv2d-6           [-1, 32, 14, 14]          18,464\n",
            "       BatchNorm2d-7           [-1, 32, 14, 14]              64\n",
            "              ReLU-8           [-1, 32, 14, 14]               0\n",
            "         AvgPool2d-9             [-1, 32, 7, 7]               0\n",
            "        Dropout2d-10             [-1, 32, 7, 7]               0\n",
            "AdaptiveAvgPool2d-11             [-1, 32, 1, 1]               0\n",
            "           Linear-12                   [-1, 10]             330\n",
            "================================================================\n",
            "Total params: 19,626\n",
            "Trainable params: 19,626\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.51\n",
            "Params size (MB): 0.07\n",
            "Estimated Total Size (MB): 1.59\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "use_batch_norm = True\n",
        "\n",
        "model = ParameterizedConvTwoD(\n",
        "    img_height=28,\n",
        "    img_width=28,\n",
        "    num_classes=10,\n",
        "    hidden_layers=hidden_layers,\n",
        "    use_batch_norm=use_batch_norm,\n",
        "    dropout_rate=0.2,\n",
        "    pool=pool,\n",
        "    use_gap=use_gap\n",
        ")\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "writer = SummaryWriter(log_dir='runs/dropout_batchnorm')\n",
        "\n",
        "training_loop(epochs=EPOCHS, criterion=criterion, model=model, optimizer=optimizer, patience=patience, writer=writer, early_stopping=early_stopping)\n",
        "summary(model, input_size=(1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Increased Stride & Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 - Model saved! Val loss improved to 1.1867\n",
            "Epoch 1/20, Train Loss: 1.5755, Train F1: 0.4231, Val Loss: 1.1867, Val F1: 0.6397\n",
            "Epoch 2/20 - Model saved! Val loss improved to 0.9934\n",
            "Epoch 2/20, Train Loss: 1.2094, Train F1: 0.5512, Val Loss: 0.9934, Val F1: 0.6559\n",
            "Epoch 3/20 - Model saved! Val loss improved to 0.8949\n",
            "Epoch 3/20, Train Loss: 1.0910, Train F1: 0.5938, Val Loss: 0.8949, Val F1: 0.7041\n",
            "Epoch 4/20 - Model saved! Val loss improved to 0.8280\n",
            "Epoch 4/20, Train Loss: 1.0102, Train F1: 0.6241, Val Loss: 0.8280, Val F1: 0.7176\n",
            "Epoch 5/20 - Model saved! Val loss improved to 0.7929\n",
            "Epoch 5/20, Train Loss: 0.9505, Train F1: 0.6480, Val Loss: 0.7929, Val F1: 0.7364\n",
            "Epoch 6/20 - Model saved! Val loss improved to 0.7226\n",
            "Epoch 6/20, Train Loss: 0.9016, Train F1: 0.6666, Val Loss: 0.7226, Val F1: 0.7510\n",
            "Epoch 7/20 - Model saved! Val loss improved to 0.6905\n",
            "Epoch 7/20, Train Loss: 0.8582, Train F1: 0.6824, Val Loss: 0.6905, Val F1: 0.7525\n",
            "Epoch 8/20 - No improvement. Patience: 1/8\n",
            "Epoch 8/20, Train Loss: 0.8292, Train F1: 0.6969, Val Loss: 0.6946, Val F1: 0.7390\n",
            "Epoch 9/20 - Model saved! Val loss improved to 0.6439\n",
            "Epoch 9/20, Train Loss: 0.8014, Train F1: 0.7067, Val Loss: 0.6439, Val F1: 0.7851\n",
            "Epoch 10/20 - Model saved! Val loss improved to 0.6225\n",
            "Epoch 10/20, Train Loss: 0.7759, Train F1: 0.7157, Val Loss: 0.6225, Val F1: 0.7862\n",
            "Epoch 11/20 - Model saved! Val loss improved to 0.6046\n",
            "Epoch 11/20, Train Loss: 0.7594, Train F1: 0.7231, Val Loss: 0.6046, Val F1: 0.7984\n",
            "Epoch 12/20 - Model saved! Val loss improved to 0.5986\n",
            "Epoch 12/20, Train Loss: 0.7418, Train F1: 0.7316, Val Loss: 0.5986, Val F1: 0.8056\n",
            "Epoch 13/20 - Model saved! Val loss improved to 0.5863\n",
            "Epoch 13/20, Train Loss: 0.7226, Train F1: 0.7389, Val Loss: 0.5863, Val F1: 0.8013\n",
            "Epoch 14/20 - Model saved! Val loss improved to 0.5648\n",
            "Epoch 14/20, Train Loss: 0.7094, Train F1: 0.7452, Val Loss: 0.5648, Val F1: 0.8095\n",
            "Epoch 15/20 - Model saved! Val loss improved to 0.5504\n",
            "Epoch 15/20, Train Loss: 0.6973, Train F1: 0.7492, Val Loss: 0.5504, Val F1: 0.8097\n",
            "Epoch 16/20 - Model saved! Val loss improved to 0.5485\n",
            "Epoch 16/20, Train Loss: 0.6852, Train F1: 0.7525, Val Loss: 0.5485, Val F1: 0.8123\n",
            "Epoch 17/20 - Model saved! Val loss improved to 0.5398\n",
            "Epoch 17/20, Train Loss: 0.6784, Train F1: 0.7580, Val Loss: 0.5398, Val F1: 0.8141\n",
            "Epoch 18/20 - Model saved! Val loss improved to 0.5298\n",
            "Epoch 18/20, Train Loss: 0.6731, Train F1: 0.7620, Val Loss: 0.5298, Val F1: 0.8150\n",
            "Epoch 19/20 - Model saved! Val loss improved to 0.5218\n",
            "Epoch 19/20, Train Loss: 0.6585, Train F1: 0.7642, Val Loss: 0.5218, Val F1: 0.8144\n",
            "Epoch 20/20 - Model saved! Val loss improved to 0.5136\n",
            "Epoch 20/20, Train Loss: 0.6489, Train F1: 0.7684, Val Loss: 0.5136, Val F1: 0.8262\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 30, 30]             640\n",
            "       BatchNorm2d-2           [-1, 64, 30, 30]             128\n",
            "              ReLU-3           [-1, 64, 30, 30]               0\n",
            "         AvgPool2d-4           [-1, 64, 15, 15]               0\n",
            "         Dropout2d-5           [-1, 64, 15, 15]               0\n",
            "            Conv2d-6           [-1, 32, 17, 17]          18,464\n",
            "       BatchNorm2d-7           [-1, 32, 17, 17]              64\n",
            "              ReLU-8           [-1, 32, 17, 17]               0\n",
            "         AvgPool2d-9             [-1, 32, 8, 8]               0\n",
            "        Dropout2d-10             [-1, 32, 8, 8]               0\n",
            "AdaptiveAvgPool2d-11             [-1, 32, 1, 1]               0\n",
            "           Linear-12                   [-1, 10]             330\n",
            "================================================================\n",
            "Total params: 19,626\n",
            "Trainable params: 19,626\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.78\n",
            "Params size (MB): 0.07\n",
            "Estimated Total Size (MB): 1.86\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "use_batch_norm = True\n",
        "\n",
        "model = ParameterizedConvTwoD(\n",
        "    img_height=28,\n",
        "    img_width=28,\n",
        "    num_classes=10,\n",
        "    stride=2,\n",
        "    padding=2,\n",
        "    hidden_layers=hidden_layers,\n",
        "    use_batch_norm=use_batch_norm,\n",
        "    dropout_rate=0.2,\n",
        "    pool=pool,\n",
        "    use_gap=use_gap\n",
        ")\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "writer = SummaryWriter(log_dir='runs/sride_padding')\n",
        "\n",
        "training_loop(epochs=EPOCHS, criterion=criterion, model=model, optimizer=optimizer, patience=patience, writer=writer, early_stopping=early_stopping)\n",
        "summary(model, input_size=(1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Set Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_on_test(model, test_loader, criterion, device, writer=None, experiment_name=\"test\"):\n",
        "    model.eval()\n",
        "    test_running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    \n",
        "    all_test_predictions = []\n",
        "    all_test_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for test_images, test_labels in test_loader:\n",
        "            test_images, test_labels = test_images.to(device), test_labels.to(device)\n",
        "            \n",
        "            test_outputs = model(test_images)\n",
        "            _, predicted = torch.max(test_outputs, dim=1)\n",
        "            \n",
        "            correct_predictions += (predicted == test_labels).sum().item()\n",
        "            total_predictions += test_labels.size(0)\n",
        "            \n",
        "            test_loss = criterion(test_outputs, test_labels)\n",
        "            test_running_loss += test_loss.item()\n",
        "            \n",
        "            all_test_predictions.extend(predicted.cpu().numpy())\n",
        "            all_test_labels.extend(test_labels.cpu().numpy())\n",
        "    \n",
        "    avg_test_loss = test_running_loss / len(test_loader)\n",
        "    test_accuracy = correct_predictions / total_predictions\n",
        "    test_f1 = f1_score(y_pred=all_test_predictions, y_true=all_test_labels, average='macro')\n",
        "    \n",
        "    # Log to TensorBoard if writer is provided\n",
        "    if writer is not None:\n",
        "        writer.add_scalar('Test/Loss', avg_test_loss, 0)\n",
        "        writer.add_scalar('Test/Accuracy', test_accuracy, 0)\n",
        "        writer.add_scalar('Test/F1', test_f1, 0)\n",
        "        writer.close()\n",
        "    \n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Test Results for {experiment_name}:\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
        "    print(f\"{'='*50}\\n\")\n",
        "    \n",
        "    return avg_test_loss, test_accuracy, test_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate Best Model on Test Set\n",
        "\n",
        "Load the best saved model and evaluate it on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded best model from models/best_model.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\macie\\AppData\\Local\\Temp\\ipykernel_16888\\1977119605.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(best_model_path))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Test Results for Final Model:\n",
            "==================================================\n",
            "Test Loss: 0.5214\n",
            "Test Accuracy: 0.8294\n",
            "Test F1 Score: 0.8248\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the best model from training\n",
        "if os.path.exists(best_model_path):\n",
        "    model.load_state_dict(torch.load(best_model_path))\n",
        "    print(f\"Loaded best model from {best_model_path}\")\n",
        "else:\n",
        "    print(\"No saved model found, using current model state\")\n",
        "\n",
        "# Create a TensorBoard writer for test results\n",
        "test_writer = SummaryWriter(log_dir='runs/test_evaluation')\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_accuracy, test_f1 = evaluate_on_test(\n",
        "    model=model,\n",
        "    test_loader=test_loader,\n",
        "    criterion=criterion,\n",
        "    device=device,\n",
        "    writer=test_writer,\n",
        "    experiment_name=\"Final Model\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What was common for all of the models, were the following parameters:\n",
        "\n",
        "```py\n",
        "LR = 0.001\n",
        "EPOCHS = 20\n",
        "hidden_layers = [64,32]\n",
        "use_batch_norm = False\n",
        "early_stopping = True\n",
        "```\n",
        "\n",
        "Then, I kept creating new models with one incremental change at a time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Baseline vs Max Pooling:\n",
        "\n",
        "For whatever reason, baseline model wasn't tracked fully and it wasn't early stopping that was the cause. Honestly, I don't know what the cause was, I rna baseline code 2 times and in both cases the tracking stopped at step 3. \n",
        "\n",
        "Despite that, charts tell us much because the loss is smaller for max pooling model and f1 score is higher. This is no evidence that the best model would always be max pooling one, but it tells us it's highly likely, I think.\n",
        "\n",
        "![Baseline - Max Pooling](./analysis/baseline_maxpooling.png)\n",
        "\n",
        "\n",
        "### Max Pooling vs Average Pooling\n",
        "\n",
        "Max pooling outperformed average pooling model on both metrics. I left average pooling for the rest of the models, which means the final best model may not be the very best that could have been but maybe thanks to this choice, the influence of other changes will be more visible.\n",
        "\n",
        "![max vs avg](./analysis/maxpool_avgpool.png)\n",
        "\n",
        "\n",
        "### Average Pooling vs Global Average Pooling\n",
        "\n",
        "Aaand the model got worse again. On both metrics. At this point I can rationally stop hoping to create the best model there could be (with at least somewhat close approximation - I wouldn't be exploring all the values of all the parameters anyway). My goal will remain purely experimental, to see how certain parameters affect the model's performance. Clearly GAP was not the right choice for this combination of parameters.\n",
        "\n",
        "![avg vs gap](./analysis/avgpool_gap.png)\n",
        "\n",
        "\n",
        "### Global Average Pooling vs Dropout & Batch Normalization\n",
        "\n",
        "Finally something more interesting, because metrics don't reveal the better model at a first glance. In terms of f1 score - training dropout and batchnorm model performed worse than gap model, but on validation set the tides reversed. That means the dropout and batchnorm model actually 'learned' instead of memorizing. Something similar is shown on the loss metric, after 7th epoch on loss val, dropout and batchnorm model got better and stayed better than the other - and that was the most important part of the loss tracking. So it ultimately won.\n",
        "\n",
        "![gap vs dropbatch](./analysis/gap_dropbatch.png)\n",
        "\n",
        "\n",
        "### Dropout & Batch Normalization vs Stride & Padding\n",
        "\n",
        "Increasing stride and padding did not turn out for the better. The model falls short to the other in terms of both f1 as well as loss.\n",
        "\n",
        "![dropbatch vs stridepad](./analysis/dropbatch_stridepad.png)\n",
        "\n",
        "\n",
        "### Test Set\n",
        "\n",
        "After all the changes, during the very last training phase, the best model was saved. We might call it best local model, not best global model, as the analysis revealed that. \n",
        "\n",
        "The final results are:\n",
        "\n",
        "F1: 0,8248\n",
        "Loss: 0,5214\n",
        "\n",
        "For comparison, our best MLP model from the previous coursework (modified-up-to-dropout-0.5-model) achieved the following results:\n",
        "\n",
        "F1: >0.895\n",
        "Loss: <0.3\n",
        "\n",
        "That means it was worse performence-wise. But I felt the potential and perhaps it indeed could've been better."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "solvroml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
